{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Flask App to expose model and other computations through an API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New model or functinlaity can be added by creating a html page and adding the function pointing to the page. Need to sun and save the word2vec model prior running the code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Phrases\n",
    "\n",
    "bigram=Phrases(data_words_nostops)\n",
    "#sg ({0, 1}, optional) â€“ Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "\n",
    "%time model = Word2Vec(bigram[data_words_nostops], size=300, window=5, min_count=5, sg=1,workers=1,iter=100)\n",
    "%time model.save(\"/data1/call_miner/Vinyas_Call_miner/call_miner/Lavanya/Tuning/Payment/Pay_word2vec_skipgram.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Change the port number (9005 to 9008) if the port is not avaliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://10.70.85.50:9007/ (Press CTRL+C to quit)\n",
      "10.72.147.207 - - [05/Jan/2019 13:17:22] \"GET /OpinionMining/ HTTP/1.1\" 200 -\n",
      "10.72.147.207 - - [05/Jan/2019 13:17:22] \"GET /OpinionMining/ HTTP/1.1\" 200 -\n",
      "10.72.147.207 - - [05/Jan/2019 13:17:26] \"GET /OpinionMining/ HTTP/1.1\" 200 -\n",
      "10.72.147.207 - - [05/Jan/2019 13:17:56] \"POST /OpinionMining/ HTTP/1.1\" 200 -\n",
      "10.72.147.207 - - [05/Jan/2019 13:18:02] \"GET /OpinionMining/ HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "from flask import render_template_string\n",
    "from gensim.models import Word2Vec\n",
    "from flask import Flask, jsonify, request\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "import collections\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config['JSON_AS_ASCII'] = False\n",
    "app.config['TEMPLATES_AUTO_RELOAD']=True\n",
    "\n",
    "def n_concordance_tokenised(text,phrase,left_margin=15,right_margin=15):\n",
    "    phraseList=phrase.split(' ')\n",
    "\n",
    "    c = nltk.ConcordanceIndex(text.tokens, key = lambda s: s.lower())\n",
    "\n",
    "    #Find the offset for each token in the phrase\n",
    "    offsets=[c.offsets(x) for x in phraseList]\n",
    "    offsets_norm=[]\n",
    "    #For each token in the phraselist, find the offsets and rebase them to the start of the phrase\n",
    "    for i in range(len(phraseList)):\n",
    "        offsets_norm.append([x-i for x in offsets[i]])\n",
    "  \n",
    "    intersects=set(offsets_norm[0]).intersection(*offsets_norm[1:])\n",
    "\n",
    "    concordance_txt = ([text.tokens[list (map(lambda x: x-left_margin if (x-left_margin)>0 else 0,[offset]))[0]:offset+len(phraseList)+right_margin]\n",
    "                    for offset in intersects])\n",
    "\n",
    "    outputs=[''.join([x+' ' for x in con_sub]) for con_sub in concordance_txt]\n",
    "    return outputs\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('home.html')\n",
    "\n",
    "\n",
    "@app.route('/', methods=['POST'])\n",
    "def word2vec():\n",
    "    model = Word2Vec.load(\"/data1/call_miner/Vinyas_Call_miner/call_miner/Lavanya/Tuning/PIU/PIU_word2vec_skipgram.model\")\n",
    "    w = request.form['text']\n",
    "    tn=request.form['topN']\n",
    "    #print tn\n",
    "    top_n=int(tn)\n",
    "    #print top_n\n",
    "    #wordcloud = WordCloud(width=1200, height=1000, margin=0,background_color=\"white\",min_font_size=10).generate_from_frequencies(model.wv.most_similar(w, topn=200)\n",
    "    w2v=pd.DataFrame(model.wv.most_similar(w, topn=top_n))\n",
    "    w2v.columns=['Word','Weigtage']\n",
    "    w2v_result=w2v.to_dict('records')\n",
    "    w2v_columnNames = w2v.columns.values\n",
    "    \n",
    "    return render_template('table.html',records=w2v_result, colnames=w2v_columnNames)\n",
    "\n",
    "@app.route('/WordAlgebra/')\n",
    "def WordAlgebra():\n",
    "    return render_template('WordAlgebra.html')\n",
    "\n",
    "@app.route('/WordAlgebra/', methods=['POST'])\n",
    "def word2vec_2():\n",
    "    model1 = Word2Vec.load(\"/data1/call_miner/Vinyas_Call_miner/call_miner/Lavanya/Tuning/PIU/PIU_word2vec_skipgram.model\")\n",
    "    add_elements = [y for y in (x.strip() for x in request.form['add'].split(',')) if y]\n",
    "    a=[x.encode('utf-8') for x in add_elements]\n",
    "    sub_elements = [y for y in (x.strip() for x in request.form['sub'].split(',')) if y]\n",
    "    s=[x.encode('utf-8') for x in sub_elements]\n",
    "    t_wa=request.form['top_wa']\n",
    "    #print tn\n",
    "    #t=int(t_wa)\n",
    "    df_wa=pd.DataFrame(model1.wv.most_similar(positive=a, negative=s, topn=int(t_wa)))\n",
    "    df_wa.columns=['Word','Weigtage']\n",
    "    df_wa_result=df_wa.to_dict('records')\n",
    "    df_wa_columnNames = df_wa.columns.values\n",
    "    \n",
    "    return render_template('table.html',records=df_wa_result, colnames=df_wa_columnNames)\n",
    "    \n",
    "    #return jsonify(model1.most_similar(positive=a, negative=s, int(t_wa)))\n",
    "\n",
    "@app.route('/WordPrefix/')\n",
    "def WordPrefix():\n",
    "    return render_template('WordPrefix.html')\n",
    "\n",
    "@app.route('/WordPrefix/', methods=['POST'])\n",
    "def get_prefix_word():\n",
    "    # load data\n",
    "    with open('/data1/call_miner/Vinyas_Call_miner/call_miner/Lavanya/Tuning/PIU/data_words_trigrams_upd.csv','rU') as f1:\n",
    "        data_words=list( csv.reader(f1) )\n",
    "    #flatten the lists\n",
    "    all_lemma_words = [y for x in data_words for y in x]\n",
    "    bgm    = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_words(all_lemma_words)\n",
    "    scored = finder.score_ngrams( bgm.likelihood_ratio  )\n",
    "    prefix_keys = collections.defaultdict(list)\n",
    "    \n",
    "    for key, scores in scored:\n",
    "        prefix_keys[key[0]].append((key[1], scores))\n",
    "    \n",
    "    for key in prefix_keys:\n",
    "        prefix_keys[key].sort(key = lambda x: -x[1])\n",
    "        \n",
    "    p_word=request.form['p_word']\n",
    "    p_length=request.form['p_length']\n",
    "    \n",
    "    l=int(p_length)\n",
    "    pw=pd.DataFrame(prefix_keys[p_word][:l])\n",
    "    pw.columns=['Word','Weigtage']\n",
    "    temp=pw.to_dict('records')\n",
    "    columnNames = pw.columns.values\n",
    "    \n",
    "    \n",
    "    #return jsonify(prefix_keys[p_word][:l])\n",
    "    return render_template('table.html',records=temp, colnames=columnNames)\n",
    "@app.route('/concordance/')\n",
    "def concordance():\n",
    "    return render_template('concordance.html')\n",
    "\n",
    "@app.route('/concordance/', methods=['POST'])\n",
    "def get_word_conc():\n",
    "    # load data\n",
    "    with open('/data1/call_miner/Vinyas_Call_miner/call_miner/Lavanya/Tuning/PIU/data_words_trigrams_upd.csv','rU') as f1:\n",
    "        data_words=list( csv.reader(f1) )\n",
    "    \n",
    "#flatten the lists\n",
    "    all_lemma_words = [y for x in data_words for y in x]\n",
    "    \n",
    "    c_word=request.form['c_word']\n",
    "    \n",
    "    p_text = nltk.Text(all_lemma_words)\n",
    "    concordance_txt=n_concordance_tokenised(p_text,c_word)\n",
    "    print c_word\n",
    "    before=[]\n",
    "    after=[]\n",
    "    center=[]\n",
    "    for i in range(len(concordance_txt)):\n",
    "        before.append(concordance_txt[i][:15])\n",
    "        center.append(concordance_txt[0][15:17])\n",
    "        after.append(concordance_txt[i][17:])\n",
    "    \n",
    "    conc_df=pd.DataFrame(list(zip(before, center, after)),columns=['Before_Words','Searched Phrase/Word', 'After_Words']) \n",
    "    recz=conc_df.to_dict('records')\n",
    "    c_columnNames = conc_df.columns.values\n",
    "    return jsonify(n_concordance_tokenised(p_text,c_word))\n",
    "    #return render_template('conc_op.html',records=recz, colnames=c_columnNames)\n",
    " \n",
    "@app.route('/OpinionMining/')\n",
    "def OpinionMining():\n",
    "    return render_template('sentiment.html')\n",
    "\n",
    "@app.route('/OpinionMining/', methods=['POST'])\n",
    "def sentiment():\n",
    "    sdf=pd.read_csv(\"/data1/call_miner/Vinyas_Call_miner/call_miner/Lavanya/Sentiment_Model/sent_score.csv\")\n",
    "    sword=request.form['s_word']\n",
    "    call_vol=len(sdf[[sword in x for x in sdf['Data_words']]])\n",
    "    sub_df=sdf[[sword in x for x in sdf['Data_words']]]\n",
    "    pos=len(sub_df.loc[sub_df['compound'] >=0.05])\n",
    "    neg=len(sub_df.loc[sub_df['compound']<=-0.05])\n",
    "    neu=len(sub_df[(sub_df['compound']> -0.05) & (sub_df['compound']<0.05)])\n",
    "    score=[call_vol,pos,neg,neu]\n",
    "    df3=pd.DataFrame(score).T\n",
    "    df3.rename(columns={0:'Total CallHit Count',1:'PositiveCount',2:'NegativeCount',3:'NeutralCount'},inplace=True)\n",
    "    \n",
    "    stemp=df3.to_dict('records')\n",
    "    s_columnNames = df3.columns.values\n",
    "    \n",
    "    \n",
    "    #return jsonify(prefix_keys[p_word][:l])\n",
    "    return render_template('table.html',records=stemp, colnames=s_columnNames)\n",
    "    \n",
    "    #return jsonify(model1.most_similar(positive=a, negative=s, int(t_wa)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run('10.70.85.50',port=9007,debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
